{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset and copying it to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from huggingface using the package datasets\n",
    "ds = load_dataset(\"yuvidhepe/us-accidents-updated\")\n",
    "\n",
    "# copying the dataset to panda\n",
    "Traffic_Accidents = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the impact on traffic in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ‘Start_Time’ and ‘End_Time’ to datetime format\n",
    "\n",
    "# remove some errors in the timestamp\n",
    "to_remove: list =  [\".000000000\", \".000000\"]\n",
    "\n",
    "for elem in to_remove:\n",
    "    Traffic_Accidents[\"Start_Time\"] = Traffic_Accidents[\"Start_Time\"].str.replace(elem, \"\")\n",
    "    Traffic_Accidents[\"End_Time\"] = Traffic_Accidents[\"End_Time\"].str.replace(elem, \"\")\n",
    "\n",
    "\n",
    "Traffic_Accidents['Start_Time'] = pd.to_datetime(Traffic_Accidents['Start_Time'], format='mixed')\n",
    "Traffic_Accidents['End_Time'] = pd.to_datetime(Traffic_Accidents['End_Time'], format='mixed')\n",
    "\n",
    "# Calculate the difference in seconds and add it as a new column\n",
    "Traffic_Accidents['Duration_Seconds'] = (Traffic_Accidents['End_Time'] - Traffic_Accidents['Start_Time']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "report = ProfileReport(Traffic_Accidents, title='Traffic Accidents - Report')\n",
    "report.to_file(\"Full Dataset.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Latitude and Longitude to H3 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "\n",
    "resolution = 7\n",
    "\n",
    "Traffic_Accidents['lat_lng'] = Traffic_Accidents.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['Start_Lat'], row['Start_Lng'], resolution),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering H3 Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Using aggregated features for clustering\n",
    "h3_features = Traffic_Accidents.groupby('lat_lng').agg(\n",
    "    accident_count=('Severity', 'size'),\n",
    "    avg_severity=('Severity', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Apply KMeans clustering on the aggregated features\n",
    "num_clusters = 10000  # Choose a suitable number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "h3_features['cluster'] = kmeans.fit_predict(h3_features[['accident_count', 'avg_severity']])\n",
    "\n",
    "# Merge cluster labels back to the main DataFrame\n",
    "Traffic_Accidents = Traffic_Accidents.merge(h3_features[['lat_lng', 'cluster']], on='lat_lng', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all discussed columns from the data set according to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_drop = [\n",
    "    'ID', 'Start_Lat', 'Street', 'Zipcode', 'End_Lng', 'Description',\n",
    "    'City', 'County', 'State', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp',\n",
    "    'Wind_Chill(F)', 'Precipitation(in)', 'Bump', 'Roundabout', 'Station', 'Turning_Loop',\n",
    "    'Sunrise_Sunset', 'Nautical_Twilight', 'Astronomical_Twilight', 'Source', 'Start_Time',\n",
    "    'End_Time', 'lat_lng', 'Start_Lng', 'End_Lat'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "Traffic_Accidents = Traffic_Accidents.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the rows with empty cells and remove all Duplicate cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Empty Cells\n",
    "Traffic_Accidents = Traffic_Accidents.dropna()\n",
    "\n",
    "# Drop Duplicate Rows\n",
    "Traffic_Accidents = Traffic_Accidents.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ProfileReport(Traffic_Accidents, title='Traffic Accidents - Report')\n",
    "report.to_file(\"Column Deletion + Drop Cells and Duplicates.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Columns with Boolean Values\n",
    "to_bool_encode = ['Amenity', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
    "\n",
    "Traffic_Accidents[to_bool_encode] = Traffic_Accidents[to_bool_encode].astype(int)\n",
    "\n",
    "# Encoding the column with 2 unique values\n",
    "Traffic_Accidents['Civil_Twilight'] = Traffic_Accidents['Civil_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "\n",
    "# Encoding all the remaining columns\n",
    "to_encode: list = [\"Wind_Direction\", \"Weather_Condition\"]\n",
    "\n",
    "Traffic_Accidents[to_encode] = Traffic_Accidents[to_encode].apply(lambda col:pd.Categorical(col).codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ProfileReport(Traffic_Accidents, title='Traffic Accidents - Report')\n",
    "report.to_file(\"After Encoding.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = Traffic_Accidents.drop('Severity', axis=1)\n",
    "\n",
    "# Target Variable\n",
    "y = Traffic_Accidents['Severity']\n",
    "\n",
    "# Splitting into train and temp \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Splitting temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsample the majority class\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the classes in the training set\n",
    "df_majority = df_train[df_train['Severity'] == 2]\n",
    "df_minority_1 = df_train[df_train['Severity'] == 1]\n",
    "df_minority_3 = df_train[df_train['Severity'] == 3]\n",
    "df_minority_4 = df_train[df_train['Severity'] == 4]\n",
    "\n",
    "# Downsample the majority class (for example, to 500,000)\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False,    \n",
    "                                    n_samples=500000, \n",
    "                                    random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the original minority classes\n",
    "df_combined = pd.concat([df_majority_downsampled, df_minority_1, df_minority_3, df_minority_4])\n",
    "\n",
    "# Step 2: Upsample minority classes using SMOTE\n",
    "X_combined = df_combined.drop('Severity', axis=1)\n",
    "y_combined = df_combined['Severity']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_normal = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X_val_normal = scaler.transform(X_val)\n",
    "\n",
    "X_test_normal = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "model = IsolationForest(contamination='auto', random_state=42)\n",
    "\n",
    "# Fit the model on the normalized resampled training data\n",
    "model.fit(X_normal)\n",
    "\n",
    "# Predict anomalies on the normalized training data\n",
    "anomalies_predictions = model.predict(X_normal)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "X_normal_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "\n",
    "# Add the anomaly predictions to the DataFrame\n",
    "X_normal_df['anomaly'] = anomalies_predictions\n",
    "\n",
    "# Extract anomalies\n",
    "anomalies = X_normal_df[X_normal_df['anomaly'] == -1]\n",
    "\n",
    "# Display the detected anomalies\n",
    "print(\"Detected anomalies:\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Train-Val-Test Split to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "train_set_df['Severity'] = y_resampled.values\n",
    "\n",
    "val_set_df = pd.DataFrame(X_val_normal, columns=X_val.columns)\n",
    "val_set_df['Severity'] = y_val.values\n",
    "\n",
    "test_set_df = pd.DataFrame(X_test_normal, columns=X_test.columns)\n",
    "test_set_df['Severity'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ProfileReport(train_set_df, title='Traffic Accidents - Report')\n",
    "report.to_file(\"Train Dataset.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting these as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df.to_csv('Train_Set.csv', index=False)\n",
    "val_set_df.to_csv('Validation_Set.csv', index=False)\n",
    "test_set_df.to_csv('Test_Set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
