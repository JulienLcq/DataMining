{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset and copying it to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from huggingface\n",
    "ds = load_dataset(\"yuvidhepe/us-accidents-updated\")\n",
    "\n",
    "Traffic_Accidents = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the impact on traffic in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some errors in the timestamp\n",
    "to_remove: list =  [\".000000000\", \".000000\"]\n",
    "\n",
    "for elem in to_remove:\n",
    "    Traffic_Accidents[\"Start_Time\"] = Traffic_Accidents[\"Start_Time\"].str.replace(elem, \"\")\n",
    "    Traffic_Accidents[\"End_Time\"] = Traffic_Accidents[\"End_Time\"].str.replace(elem, \"\")\n",
    "\n",
    "# Convert ‘Start_Time’ and ‘End_Time’ to datetime format\n",
    "Traffic_Accidents['Start_Time'] = pd.to_datetime(Traffic_Accidents['Start_Time'], format='mixed')\n",
    "Traffic_Accidents['End_Time'] = pd.to_datetime(Traffic_Accidents['End_Time'], format='mixed')\n",
    "\n",
    "# Calculate the difference in seconds and add it as a new column\n",
    "Traffic_Accidents['Duration_Seconds'] = (Traffic_Accidents['End_Time'] - Traffic_Accidents['Start_Time']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning 'Wind_Direction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traffic_Accidents.loc[Traffic_Accidents['Wind_Direction']=='Calm', 'Wind_Direction'] = 'CALM'\n",
    "Traffic_Accidents.loc[Traffic_Accidents['Wind_Direction']=='Variable', 'Wind_Direction'] = 'VAR'\n",
    "Traffic_Accidents.loc[Traffic_Accidents['Wind_Direction']=='North', 'Wind_Direction'] = 'N'\n",
    "Traffic_Accidents.loc[Traffic_Accidents['Wind_Direction']=='East', 'Wind_Direction'] = 'E'\n",
    "Traffic_Accidents.loc[Traffic_Accidents['Wind_Direction']=='South', 'Wind_Direction'] = 'S'\n",
    "Traffic_Accidents.loc[Traffic_Accidents['Wind_Direction']=='West', 'Wind_Direction'] = 'W'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Latitude and Longitude to H3 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "\n",
    "resolution = 7\n",
    "\n",
    "Traffic_Accidents['lat_lng'] = Traffic_Accidents.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['Start_Lat'], row['Start_Lng'], resolution),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering H3 Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Using aggregated features for clustering\n",
    "h3_features = Traffic_Accidents.groupby('lat_lng').agg(\n",
    "    accident_count=('Severity', 'size'),\n",
    "    avg_severity=('Severity', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Apply KMeans clustering on the aggregated features\n",
    "num_clusters = 10000  \n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "h3_features['Cluster'] = kmeans.fit_predict(h3_features[['accident_count', 'avg_severity']])\n",
    "\n",
    "# Merge cluster labels back to the main DataFrame\n",
    "Traffic_Accidents = Traffic_Accidents.merge(h3_features[['lat_lng', 'Cluster']], on='lat_lng', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all discussed columns from the data set according to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_drop = [\n",
    "    'ID', 'Start_Lat', 'Street', 'Zipcode', 'End_Lng', 'Description',\n",
    "    'City', 'County', 'State', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp',\n",
    "    'Wind_Chill(F)', 'Precipitation(in)', 'Bump', 'Roundabout', 'Station', 'Turning_Loop',\n",
    "    'Sunrise_Sunset', 'Nautical_Twilight', 'Astronomical_Twilight', 'Source', 'Start_Time',\n",
    "    'End_Time', 'lat_lng', 'Start_Lng', 'End_Lat'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "Traffic_Accidents = Traffic_Accidents.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill missing values of numerical data with their Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_fill = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "Traffic_Accidents[features_to_fill] = Traffic_Accidents[features_to_fill].fillna(Traffic_Accidents[features_to_fill].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the rows with empty cells and remove all Duplicate cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Empty Cells\n",
    "Traffic_Accidents = Traffic_Accidents.dropna()\n",
    "\n",
    "# Drop Duplicate Rows\n",
    "Traffic_Accidents = Traffic_Accidents.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Columns with Boolean Values\n",
    "to_bool_encode = ['Amenity', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
    "\n",
    "Traffic_Accidents[to_bool_encode] = Traffic_Accidents[to_bool_encode].astype(int)\n",
    "\n",
    "# Encoding the column with 2 unique values\n",
    "Traffic_Accidents['Civil_Twilight'] = Traffic_Accidents['Civil_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "\n",
    "# Encoding all the remaining columns\n",
    "to_encode: list = [\"Wind_Direction\", \"Weather_Condition\"]\n",
    "\n",
    "Traffic_Accidents[to_encode] = Traffic_Accidents[to_encode].apply(lambda col:pd.Categorical(col).codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = Traffic_Accidents.drop('Severity', axis=1)\n",
    "\n",
    "# Target Variable\n",
    "y = Traffic_Accidents['Severity']\n",
    "\n",
    "# Splitting into train and temp \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Splitting temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsample the majority class\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the classes in the training set\n",
    "df_majority = df_train[df_train['Severity'] == 2]\n",
    "df_minority_1 = df_train[df_train['Severity'] == 1]\n",
    "df_minority_3 = df_train[df_train['Severity'] == 3]\n",
    "df_minority_4 = df_train[df_train['Severity'] == 4]\n",
    "\n",
    "# Downsample the majority class to 500000\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False,    \n",
    "                                    n_samples=500000, \n",
    "                                    random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the original minority classes\n",
    "df_combined = pd.concat([df_majority_downsampled, df_minority_1, df_minority_3, df_minority_4])\n",
    "\n",
    "# Upsample minority classes using SMOTE\n",
    "X_combined = df_combined.drop('Severity', axis=1)\n",
    "y_combined = df_combined['Severity']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_normal = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X_val_normal = scaler.transform(X_val)\n",
    "\n",
    "X_test_normal = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected anomalies:\n",
      "         Distance(mi)  Temperature(F)  Humidity(%)  Pressure(in)  \\\n",
      "9            0.592043       -1.906197     1.188614      0.415315   \n",
      "10           0.245288        1.138515    -0.449308      0.577926   \n",
      "13          -0.306301        1.927885    -2.485643     -0.824590   \n",
      "21          -0.310203       -1.624279     1.188614     -0.580674   \n",
      "22          -0.294109       -0.158307    -2.042961     -0.824590   \n",
      "...               ...             ...          ...           ...   \n",
      "3512315      3.269981       -0.230232     1.514524     -1.047528   \n",
      "3512324      0.005548        1.763443    -2.194083      0.264566   \n",
      "3512337      5.534320       -1.852588     1.008666     -0.343200   \n",
      "3512349      0.012144       -2.007389     1.029717     -3.105357   \n",
      "3512387      8.754662       -1.092606     0.545388     -0.448419   \n",
      "\n",
      "         Visibility(mi)  Wind_Direction  Wind_Speed(mph)  Weather_Condition  \\\n",
      "9             -1.778025       -1.464518        -1.621729           1.325493   \n",
      "10             0.335920        1.212831        -0.875249           1.160446   \n",
      "13             0.335920        0.830353         0.511073          -0.737595   \n",
      "21            -0.086869        0.256635         0.297793          -0.737595   \n",
      "22             0.335920       -1.464518        -1.621729          -0.737595   \n",
      "...                 ...             ...              ...                ...   \n",
      "3512315       -2.808996        0.256635        -0.555328          -0.930150   \n",
      "3512324        0.335920       -1.273279         2.104545           1.242969   \n",
      "3512337       -0.375664       -0.699561         0.578668           1.077922   \n",
      "3512349       -2.493604       -1.273279        -1.348319           0.830352   \n",
      "3512387       -0.597615       -0.508322         1.027814          -0.957658   \n",
      "\n",
      "          Amenity  Crossing  ...  Junction   No_Exit   Railway       Stop  \\\n",
      "9       -0.062955  3.782178  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "10      -0.062955 -0.264398  ...  4.599353 -0.030568 -0.060913  -0.093594   \n",
      "13      -0.062955  3.782178  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "21      -0.062955  3.782178  ... -0.217422 -0.030568 -0.060913  10.684410   \n",
      "22      -0.062955  3.782178  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "...           ...       ...  ...       ...       ...       ...        ...   \n",
      "3512315 -0.062955 -0.264398  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "3512324 -0.062955 -0.264398  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "3512337 -0.062955 -0.264398  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "3512349 -0.062955 -0.264398  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "3512387 -0.062955 -0.264398  ... -0.217422 -0.030568 -0.060913  -0.093594   \n",
      "\n",
      "         Traffic_Calming  Traffic_Signal  Civil_Twilight  Duration_Seconds  \\\n",
      "9              -0.019153         2.98597        0.713652         -0.030792   \n",
      "10             -0.019153        -0.33490        0.713652         -0.011366   \n",
      "13             -0.019153         2.98597        0.713652         -0.030471   \n",
      "21             -0.019153        -0.33490       -1.401243         -0.031869   \n",
      "22             -0.019153         2.98597       -1.401243          0.048888   \n",
      "...                  ...             ...             ...               ...   \n",
      "3512315        -0.019153        -0.33490        0.713652         -0.015489   \n",
      "3512324        -0.019153         2.98597        0.713652         -0.034078   \n",
      "3512337        -0.019153        -0.33490       -1.401243         -0.034048   \n",
      "3512349        -0.019153        -0.33490       -1.401243         -0.000132   \n",
      "3512387        -0.019153        -0.33490       -1.401243         -0.018218   \n",
      "\n",
      "          Cluster  anomaly  \n",
      "9       -0.676217       -1  \n",
      "10      -0.738022       -1  \n",
      "13       1.307444       -1  \n",
      "21      -1.237322       -1  \n",
      "22      -1.046699       -1  \n",
      "...           ...      ...  \n",
      "3512315  0.279677       -1  \n",
      "3512324  1.347722       -1  \n",
      "3512337 -1.223086       -1  \n",
      "3512349  0.410231       -1  \n",
      "3512387  1.652233       -1  \n",
      "\n",
      "[309212 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "model = IsolationForest(contamination='auto', random_state=42)\n",
    "\n",
    "model.fit(X_normal)\n",
    "\n",
    "# Predict anomalies on the normalized training data\n",
    "anomalies_predictions = model.predict(X_normal)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "X_normal_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "\n",
    "# Add the anomaly predictions to the DataFrame\n",
    "X_normal_df['anomaly'] = anomalies_predictions\n",
    "\n",
    "# Extract anomalies\n",
    "anomalies = X_normal_df[X_normal_df['anomaly'] == -1]\n",
    "\n",
    "# Display the detected anomalies\n",
    "print(\"Detected anomalies:\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Train-Val-Test Split to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "train_set_df['Severity'] = y_resampled.values\n",
    "\n",
    "val_set_df = pd.DataFrame(X_val_normal, columns=X_val.columns)\n",
    "val_set_df['Severity'] = y_val.values\n",
    "\n",
    "test_set_df = pd.DataFrame(X_test_normal, columns=X_test.columns)\n",
    "test_set_df['Severity'] = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting these as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df.to_csv('Train_Set.csv', index=False)\n",
    "val_set_df.to_csv('Validation_Set.csv', index=False)\n",
    "test_set_df.to_csv('Test_Set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
