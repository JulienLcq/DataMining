{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset and copying it to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from huggingface using the package datasets\n",
    "ds = load_dataset(\"yuvidhepe/us-accidents-updated\")\n",
    "\n",
    "# copying the dataset to panda\n",
    "Traffic_Accidents = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the impact on traffic in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ‘Start_Time’ and ‘End_Time’ to datetime format\n",
    "\n",
    "# remove some errors in the timestamp\n",
    "to_remove: list =  [\".000000000\", \".000000\"]\n",
    "\n",
    "for elem in to_remove:\n",
    "    Traffic_Accidents[\"Start_Time\"] = Traffic_Accidents[\"Start_Time\"].str.replace(elem, \"\")\n",
    "    Traffic_Accidents[\"End_Time\"] = Traffic_Accidents[\"End_Time\"].str.replace(elem, \"\")\n",
    "\n",
    "\n",
    "Traffic_Accidents['Start_Time'] = pd.to_datetime(Traffic_Accidents['Start_Time'], format='mixed')\n",
    "Traffic_Accidents['End_Time'] = pd.to_datetime(Traffic_Accidents['End_Time'], format='mixed')\n",
    "\n",
    "# Calculate the difference in seconds and add it as a new column\n",
    "Traffic_Accidents['Duration_Seconds'] = (Traffic_Accidents['End_Time'] - Traffic_Accidents['Start_Time']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Latitude and Longitude to H3 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "\n",
    "resolution = 7\n",
    "\n",
    "Traffic_Accidents['lat_lng'] = Traffic_Accidents.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['Start_Lat'], row['Start_Lng'], resolution),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering H3 Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Using aggregated features for clustering\n",
    "h3_features = Traffic_Accidents.groupby('lat_lng').agg(\n",
    "    accident_count=('Severity', 'size'),\n",
    "    avg_severity=('Severity', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Apply KMeans clustering on the aggregated features\n",
    "num_clusters = 10000  # Choose a suitable number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "h3_features['cluster'] = kmeans.fit_predict(h3_features[['accident_count', 'avg_severity']])\n",
    "\n",
    "# Merge cluster labels back to the main DataFrame\n",
    "Traffic_Accidents = Traffic_Accidents.merge(h3_features[['lat_lng', 'cluster']], on='lat_lng', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all discussed columns from the data set according to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_drop = [\n",
    "    'ID', 'Start_Lat', 'Street', 'Zipcode', 'End_Lng', 'Description',\n",
    "    'City', 'County', 'State', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp',\n",
    "    'Wind_Chill(F)', 'Precipitation(in)', 'Bump', 'Roundabout', 'Station', 'Turning_Loop',\n",
    "    'Sunrise_Sunset', 'Nautical_Twilight', 'Astronomical_Twilight', 'Source', 'Start_Time',\n",
    "    'End_Time', 'lat_lng', 'Start_Lng', 'End_Lat'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "Traffic_Accidents = Traffic_Accidents.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the rows with empty cells and remove all Duplicate cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Empty Cells\n",
    "Traffic_Accidents = Traffic_Accidents.dropna()\n",
    "\n",
    "# Drop Duplicate Rows\n",
    "Traffic_Accidents = Traffic_Accidents.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Columns with Boolean Values\n",
    "to_bool_encode = ['Amenity', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
    "\n",
    "Traffic_Accidents[to_bool_encode] = Traffic_Accidents[to_bool_encode].astype(int)\n",
    "\n",
    "# Encoding the column with 2 unique values\n",
    "Traffic_Accidents['Civil_Twilight'] = Traffic_Accidents['Civil_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "\n",
    "# Encoding all the remaining columns\n",
    "to_encode: list = [\"Wind_Direction\", \"Weather_Condition\"]\n",
    "\n",
    "Traffic_Accidents[to_encode] = Traffic_Accidents[to_encode].apply(lambda col:pd.Categorical(col).codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = Traffic_Accidents.drop('Severity', axis=1)\n",
    "\n",
    "# Target Variable\n",
    "y = Traffic_Accidents['Severity']\n",
    "\n",
    "# Splitting into train and temp \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Splitting temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsample the majority class\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the classes in the training set\n",
    "df_majority = df_train[df_train['Severity'] == 2]\n",
    "df_minority_1 = df_train[df_train['Severity'] == 1]\n",
    "df_minority_3 = df_train[df_train['Severity'] == 3]\n",
    "df_minority_4 = df_train[df_train['Severity'] == 4]\n",
    "\n",
    "# Downsample the majority class (for example, to 500,000)\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False,    \n",
    "                                    n_samples=500000, \n",
    "                                    random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the original minority classes\n",
    "df_combined = pd.concat([df_majority_downsampled, df_minority_1, df_minority_3, df_minority_4])\n",
    "\n",
    "# Step 2: Upsample minority classes using SMOTE\n",
    "X_combined = df_combined.drop('Severity', axis=1)\n",
    "y_combined = df_combined['Severity']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_normal = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X_val_normal = scaler.transform(X_val)\n",
    "\n",
    "X_test_normal = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected anomalies:\n",
      "         Distance(mi)  Temperature(F)  Humidity(%)  Pressure(in)  \\\n",
      "17          -0.320130       -0.673885     1.075413     -0.089652   \n",
      "19           0.805101       -1.066063    -0.694228     -0.834971   \n",
      "26          -0.320130        0.110470     1.119654      0.655667   \n",
      "35           1.122937       -2.186570     1.252377     -4.472129   \n",
      "40           0.049926       -0.673885     1.252377      0.417165   \n",
      "...               ...             ...          ...           ...   \n",
      "3158535      0.929846       -2.658199     1.095526     -7.540080   \n",
      "3158571      3.989056       -1.784952     1.184281     -2.208368   \n",
      "3158600      0.855143        0.291408     1.173784      0.389741   \n",
      "3158611      4.406141        0.321023     1.292506      0.615917   \n",
      "3158616      7.629597       -2.150002    -0.487849     -2.339243   \n",
      "\n",
      "         Visibility(mi)  Wind_Direction  Wind_Speed(mph)  Weather_Condition  \\\n",
      "17             0.330375       -1.576502        -1.569330           1.331650   \n",
      "19             0.330375        0.753653        -0.334424          -0.968187   \n",
      "26             0.330375       -1.576502        -1.569330           1.163370   \n",
      "35            -0.093552        0.132279         0.283028           0.742668   \n",
      "40            -1.789263        1.219684        -0.149189           0.518293   \n",
      "...                 ...             ...              ...                ...   \n",
      "3158535       -2.583183       -0.333752        -0.188484           0.742668   \n",
      "3158571        0.330375       -1.421158        -0.854773          -0.771859   \n",
      "3158600       -1.109553        0.908997        -0.089968           1.247510   \n",
      "3158611       -1.325934       -1.110471        -0.154449           1.724305   \n",
      "3158616        0.330375       -1.265814        -0.334424          -0.827953   \n",
      "\n",
      "          Amenity  Crossing  ...  Junction   No_Exit    Railway      Stop  \\\n",
      "17      -0.064978  3.704849  ... -0.218395 -0.031715  16.020597 -0.096902   \n",
      "19      -0.064978 -0.269917  ...  4.578850 -0.031715  -0.062420 -0.096902   \n",
      "26      -0.064978  3.704849  ... -0.218395 -0.031715  -0.062420 -0.096902   \n",
      "35      -0.064978 -0.269917  ... -0.218395 -0.031715  -0.062420 -0.096902   \n",
      "40      -0.064978 -0.269917  ...  4.578850 -0.031715  -0.062420 -0.096902   \n",
      "...           ...       ...  ...       ...       ...        ...       ...   \n",
      "3158535 -0.064978 -0.269917  ... -0.218395 -0.031715  -0.062420 -0.096902   \n",
      "3158571 -0.064978 -0.269917  ... -0.218395 -0.031715  -0.062420 -0.096902   \n",
      "3158600 -0.064978 -0.269917  ...  4.578850 -0.031715  -0.062420 -0.096902   \n",
      "3158611 -0.064978 -0.269917  ... -0.218395 -0.031715  -0.062420 -0.096902   \n",
      "3158616 -0.064978 -0.269917  ... -0.218395 -0.031715  -0.062420 -0.096902   \n",
      "\n",
      "         Traffic_Calming  Traffic_Signal  Civil_Twilight  Duration_Seconds  \\\n",
      "17             -0.019454       -0.338826        0.697999         -0.033107   \n",
      "19             -0.019454       -0.338826       -1.432666         -0.035979   \n",
      "26             -0.019454        2.951369       -1.432666         -0.033099   \n",
      "35             -0.019454       -0.338826       -1.432666         -0.035979   \n",
      "40             -0.019454       -0.338826        0.697999         -0.014039   \n",
      "...                  ...             ...             ...               ...   \n",
      "3158535        -0.019454       -0.338826       -1.432666         -0.035079   \n",
      "3158571        -0.019454       -0.338826       -1.432666         -0.003562   \n",
      "3158600        -0.019454       -0.338826       -1.432666         -0.035060   \n",
      "3158611        -0.019454       -0.338826       -1.432666         -0.014039   \n",
      "3158616        -0.019454       -0.338826        0.697999         -0.027253   \n",
      "\n",
      "          cluster  anomaly  \n",
      "17      -0.184567       -1  \n",
      "19       0.278996       -1  \n",
      "26      -0.109160       -1  \n",
      "35       0.645608       -1  \n",
      "40       0.414173       -1  \n",
      "...           ...      ...  \n",
      "3158535 -1.157216       -1  \n",
      "3158571  1.876448       -1  \n",
      "3158600  0.410003       -1  \n",
      "3158611  0.095864       -1  \n",
      "3158616  0.711632       -1  \n",
      "\n",
      "[271482 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "model = IsolationForest(contamination='auto', random_state=42)\n",
    "\n",
    "# Fit the model on the normalized resampled training data\n",
    "model.fit(X_normal)\n",
    "\n",
    "# Predict anomalies on the normalized training data\n",
    "anomalies_predictions = model.predict(X_normal)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "X_normal_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "\n",
    "# Add the anomaly predictions to the DataFrame\n",
    "X_normal_df['anomaly'] = anomalies_predictions\n",
    "\n",
    "# Extract anomalies\n",
    "anomalies = X_normal_df[X_normal_df['anomaly'] == -1]\n",
    "\n",
    "# Display the detected anomalies\n",
    "print(\"Detected anomalies:\")\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Train-Val-Test Split to Dataframe and Exporting as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "train_set_df['Severity'] = y_resampled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_df = pd.DataFrame(X_val_normal, columns=X_val.columns)\n",
    "val_set_df['Severity'] = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = pd.DataFrame(X_test_normal, columns=X_test.columns)\n",
    "test_set_df['Severity'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df.to_csv('Train_Set.csv', index=False)\n",
    "val_set_df.to_csv('Validation_Set.csv', index=False)\n",
    "test_set_df.to_csv('Test_Set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
