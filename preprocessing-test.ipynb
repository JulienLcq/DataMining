{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset and copying it to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from huggingface using the package datasets\n",
    "ds = load_dataset(\"yuvidhepe/us-accidents-updated\")\n",
    "\n",
    "# copying the dataset to panda\n",
    "Traffic_Accidents = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the impact on traffic in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ‘Start_Time’ and ‘End_Time’ to datetime format\n",
    "Traffic_Accidents['Start_Time'] = pd.to_datetime(Traffic_Accidents['Start_Time'], format='mixed')\n",
    "Traffic_Accidents['End_Time'] = pd.to_datetime(Traffic_Accidents['End_Time'], format='mixed')\n",
    "\n",
    "# Calculate the difference in seconds and add it as a new column\n",
    "Traffic_Accidents['Duration_Seconds'] = (Traffic_Accidents['End_Time'] - Traffic_Accidents['Start_Time']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete all discussed columns from the data set according to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_drop = [\n",
    "    'ID', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Description',\n",
    "    'City', 'County', 'State', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp',\n",
    "    'Wind_Chill(F)', 'Precipitation(in)', 'Bump', 'Roundabout', 'Station', 'Turning_Loop',\n",
    "    'Sunrise_Sunset', 'Nautical_Twilight', 'Astronomical_Twilight', 'Source', 'Start_Time',\n",
    "    'End_Time'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "Traffic_Accidents = Traffic_Accidents.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the rows with empty cells and remove all Duplicate cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Empty Cells\n",
    "Traffic_Accidents = Traffic_Accidents.dropna()\n",
    "\n",
    "# Drop Duplicate Rows\n",
    "Traffic_Accidents = Traffic_Accidents.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Columns with Boolean Values\n",
    "to_bool_encode = ['Amenity', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
    "\n",
    "Traffic_Accidents[to_bool_encode] = Traffic_Accidents[to_bool_encode].astype(int)\n",
    "\n",
    "# Encoding the column with 2 unique values\n",
    "Traffic_Accidents['Civil_Twilight'] = Traffic_Accidents['Civil_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "\n",
    "# Encoding all the remaining columns\n",
    "to_encode: list = [\"Street\", \"Wind_Direction\", \"Weather_Condition\", \"Zipcode\"]\n",
    "\n",
    "Traffic_Accidents[to_encode] = Traffic_Accidents[to_encode].apply(lambda col:pd.Categorical(col).codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = Traffic_Accidents.drop('Severity', axis=1)\n",
    "\n",
    "# Target Variable\n",
    "y = Traffic_Accidents['Severity']\n",
    "\n",
    "# Splitting into train and temp \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Splitting temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsample the majority class\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the classes in the training set\n",
    "df_majority = df_train[df_train['Severity'] == 2]\n",
    "df_minority_1 = df_train[df_train['Severity'] == 1]\n",
    "df_minority_3 = df_train[df_train['Severity'] == 3]\n",
    "df_minority_4 = df_train[df_train['Severity'] == 4]\n",
    "\n",
    "# Downsample the majority class (for example, to 200,000)\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False,    \n",
    "                                    n_samples=2000000, \n",
    "                                    random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the original minority classes\n",
    "df_combined = pd.concat([df_majority_downsampled, df_minority_1, df_minority_3, df_minority_4])\n",
    "\n",
    "# Step 2: Upsample minority classes using SMOTE\n",
    "X_combined = df_combined.drop('Severity', axis=1)\n",
    "y_combined = df_combined['Severity']\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_normal = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X_val_normal = scaler.transform(X_val)\n",
    "\n",
    "X_test_normal = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "model = IsolationForest(contamination='auto', random_state=42)\n",
    "\n",
    "# Fit the model on the normalized resampled training data\n",
    "model.fit(X_normal)\n",
    "\n",
    "# Predict anomalies on the normalized training data\n",
    "anomalies_predictions = model.predict(X_normal)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "X_normal_df = pd.DataFrame(X_normal, columns=X_combined.columns)\n",
    "\n",
    "# Add the anomaly predictions to the DataFrame\n",
    "X_normal_df['anomaly'] = anomalies_predictions\n",
    "\n",
    "# Extract anomalies\n",
    "anomalies = X_normal_df[X_normal_df['anomaly'] == -1]\n",
    "\n",
    "# Display the detected anomalies\n",
    "print(\"Detected anomalies:\")\n",
    "print(anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
