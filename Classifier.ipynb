{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a6c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7719\n",
      "Decision Tree Recall: 0.7720\n",
      "Decision Tree F1 Score: 0.7715\n",
      "Random Forest Accuracy: 0.8451\n",
      "Random Forest Recall: 0.8453\n",
      "Random Forest F1 Score: 0.8420\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got [1 2 3 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m xgb_predictions \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     53\u001b[0m xgb_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, xgb_predictions)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got [1 2 3 4]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "train_set_df = pd.read_csv(\"Train_set.csv\")\n",
    "\n",
    "\n",
    "X_train = train_set_df.drop(columns=[\"Severity\"])\n",
    "y_train = train_set_df[\"Severity\"]\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('Test_Set.csv')\n",
    "\n",
    "# Separate features and target variable for the test set\n",
    "X_test = test_data.drop(columns=[\"Severity\"])\n",
    "y_test = test_data[\"Severity\"]\n",
    "\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "dt_recall = recall_score(y_test, dt_predictions, average='macro')\n",
    "dt_f1 = f1_score(y_test, dt_predictions, average='macro')\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Recall: {dt_recall:.4f}\")\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "train_set_df = pd.read_csv(\"Train_set.csv\")\n",
    "\n",
    "X_train = train_set_df.drop(columns=[\"Severity\"])\n",
    "y_train = train_set_df[\"Severity\"]\n",
    "\n",
    "\n",
    "train_set_df = pd.read_csv(\"Test_set.csv\")\n",
    "\n",
    "X_test = train_set_df.drop(columns=[\"Severity\"])\n",
    "y_test = train_set_df[\"Severity\"]\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='macro')\n",
    "rf_f1 = f1_score(y_test, rf_predictions, average='macro')\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest Recall: {rf_recall:.4f}\")\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.4740\n",
      "XGBoost Recall: 0.3327\n",
      "XGBoost F1 Score: 0.3224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "train_set_df = pd.read_csv(\"Train_set.csv\")\n",
    "\n",
    "\n",
    "X_train = train_set_df.drop(columns=[\"Severity\"])\n",
    "y_train = label_encoder.fit_transform(train_set_df[\"Severity\"])\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('Test_Set.csv')\n",
    "\n",
    "# Separate features and target variable for the test set\n",
    "X_test = test_data.drop(columns=[\"Severity\"])\n",
    "y_test = label_encoder.fit_transform(train_set_df[\"Severity\"])\n",
    "\n",
    "\n",
    "size: int = min(len(X_test), len(X_train))\n",
    "\n",
    "X_train, y_train, X_test, y_test = X_train[:size], y_train[:size], X_test[:size], y_test[:size]\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "xgb_recall = recall_score(y_test, xgb_predictions, average='macro')\n",
    "xgb_f1 = f1_score(y_test, xgb_predictions, average='macro')\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"XGBoost Recall: {xgb_recall:.4f}\")\n",
    "print(f\"XGBoost F1 Score: {xgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b51a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "train_set_df = pd.read_csv(\"Train_set.csv\")\n",
    "\n",
    "X_train = train_set_df.drop(columns=[\"Severity\"])\n",
    "y_train = train_set_df[\"Severity\"]\n",
    "\n",
    "\n",
    "train_set_df = pd.read_csv(\"Test_set.csv\")\n",
    "\n",
    "X_test = train_set_df.drop(columns=[\"Severity\"])\n",
    "y_test = train_set_df[\"Severity\"]\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='relu', max_iter=1000, random_state=42)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "predicted_class = mlp.predict(X_test)\n",
    "print(f'Predicted class for the sample: {predicted_class[0]}')\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
